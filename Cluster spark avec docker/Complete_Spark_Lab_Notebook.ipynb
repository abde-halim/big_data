{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed9c15e6",
   "metadata": {},
   "source": [
    "## Summary: All Lab Pillars Covered\n",
    "\n",
    "✅ **Pillar 1: Installation & Setup** - Docker cluster with Hadoop and Spark  \n",
    "✅ **Pillar 2: SparkPi Example** - Calculate PI with different partitions  \n",
    "✅ **Pillar 3: Scala WordCount** - spark-shell RDD operations  \n",
    "✅ **Pillar 4: Python Applications** - RDD and DataFrame based implementations  \n",
    "✅ **Pillar 5: PySpark on Colab** - Complete installation and setup steps  \n",
    "✅ **Pillar 6: Data Manipulation** - CSV loading, filtering, groupBy, sorting  \n",
    "✅ **Pillar 7: Spark SQL** - SQL queries with aggregations and joins  \n",
    "✅ **Pillar 8: MongoDB Integration** - Atlas connection, CRUD, aggregations  \n",
    "✅ **Pillar 9: Visualization** - Barplots, histograms, status comparisons  \n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Spark is a unified framework** for batch and stream processing\n",
    "2. **RDDs** are low-level, **DataFrames** are optimized via Catalyst\n",
    "3. **Spark SQL** provides SQL interface for complex queries\n",
    "4. **MongoDB integration** enables NoSQL data analysis\n",
    "5. **Visualization** makes insights actionable\n",
    "6. **Colab** allows free Spark experimentation\n",
    "\n",
    "### Files Generated\n",
    "\n",
    "- `wordcount_analysis.py` - Exercise 1 (DataFrame + SQL)\n",
    "- `dataframe_agg.py` - Exercise 2 (aggregations)\n",
    "- `dataframe_join.py` - Exercise 3 (joins)\n",
    "- `mongodb_integration.py` - Exercise 4 (MongoDB)\n",
    "- This complete notebook covering all pillars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e855a022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.4 Advanced visualization - Amount by Type and Status\n",
    "df_detailed = df.groupBy(\"Transaction Type\", \"Transaction Status\").agg(spark_sum(\"Amount\")).toPandas()\n",
    "df_detailed.columns = [\"Transaction Type\", \"Transaction Status\", \"Total Amount\"]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=df_detailed, x=\"Transaction Type\", y=\"Total Amount\", hue=\"Transaction Status\", palette=\"Set2\")\n",
    "plt.title(\"Amount by Transaction Type and Status\", fontsize=14, fontweight='bold')\n",
    "plt.xlabel(\"Transaction Type\", fontsize=12)\n",
    "plt.ylabel(\"Total Amount (€)\", fontsize=12)\n",
    "plt.legend(title=\"Status\", loc=\"upper right\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Advanced visualization created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db96af50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.3 Successful vs Failed Transactions (Countplot)\n",
    "df_status = df.groupBy(\"Transaction Status\").count().toPandas()\n",
    "df_status.columns = [\"Transaction Status\", \"Count\"]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(data=df_status, x=\"Transaction Status\", y=\"Count\", palette=\"pastel\")\n",
    "plt.title(\"Successful vs Failed Transactions\", fontsize=14, fontweight='bold')\n",
    "plt.xlabel(\"Transaction Status\", fontsize=12)\n",
    "plt.ylabel(\"Number of Transactions\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Status comparison plot created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e57a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.2 Distribution of transaction amounts (Histogram)\n",
    "df_pandas = df.select(\"Amount\").toPandas()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(df_pandas[\"Amount\"], bins=30, kde=True, color=\"skyblue\", edgecolor=\"black\")\n",
    "plt.title(\"Distribution of Transaction Amounts\", fontsize=14, fontweight='bold')\n",
    "plt.xlabel(\"Amount (€)\", fontsize=12)\n",
    "plt.ylabel(\"Frequency\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Histogram created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b622f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.1 Convert Spark DF to Pandas and visualize\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Aggregate data by transaction type\n",
    "df_grouped = df.groupBy(\"Transaction Type\").agg(spark_sum(\"Amount\")).toPandas()\n",
    "df_grouped.columns = [\"Transaction Type\", \"Total Amount\"]\n",
    "df_grouped = df_grouped.sort_values(by=\"Total Amount\", ascending=False)\n",
    "\n",
    "# Create barplot\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(data=df_grouped, x=\"Transaction Type\", y=\"Total Amount\", palette=\"coolwarm\")\n",
    "plt.title(\"Total Amount by Transaction Type\", fontsize=14, fontweight='bold')\n",
    "plt.xlabel(\"Transaction Type\", fontsize=12)\n",
    "plt.ylabel(\"Total Amount (€)\", fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Barplot created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b526e9",
   "metadata": {},
   "source": [
    "## Section 8: Data Visualization with Matplotlib & Seaborn\n",
    "\n",
    "### 8.1 Visualizing Total Amount by Transaction Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2c671a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.3 Aggregation - Accounts with multiple transactions\n",
    "print(\"\\n=== Accounts with More Than 2 Transactions ===\")\n",
    "pipeline2 = [\n",
    "    {\n",
    "        \"$group\": {\n",
    "            \"_id\": \"$Sender Account ID\",\n",
    "            \"count\": {\"$sum\": 1},\n",
    "            \"total_amount\": {\"$sum\": \"$Amount\"}\n",
    "        }\n",
    "    },\n",
    "    {\"$match\": {\"count\": {\"$gt\": 2}}},\n",
    "    {\"$sort\": {\"count\": -1}}\n",
    "]\n",
    "\n",
    "result2 = transactions_col.aggregate(pipeline2)\n",
    "for doc in result2:\n",
    "    print(f\"Account: {doc['_id']}, Transactions: {doc['count']}, Total: ${doc['total_amount']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bcf210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.2 Aggregation - Average amount by transaction type\n",
    "print(\"\\n=== MongoDB Aggregation: Average Amount by Type ===\")\n",
    "pipeline = [\n",
    "    {\n",
    "        \"$group\": {\n",
    "            \"_id\": \"$Transaction Type\",\n",
    "            \"avg_amount\": {\"$avg\": \"$Amount\"},\n",
    "            \"count\": {\"$sum\": 1},\n",
    "            \"total\": {\"$sum\": \"$Amount\"}\n",
    "        }\n",
    "    },\n",
    "    {\"$sort\": {\"total\": -1}}\n",
    "]\n",
    "\n",
    "result = transactions_col.aggregate(pipeline)\n",
    "for doc in result:\n",
    "    print(f\"Type: {doc['_id']}, Avg: ${doc['avg_amount']:.2f}, Count: {doc['count']}, Total: ${doc['total']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b19c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1 Insert transaction data into MongoDB Atlas\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# MongoDB Atlas connection\n",
    "MONGO_URI = \"mongodb+srv://abdelilahhalim05_db_user:C2X4AXnO7MJWm52Z@cluster0.htrgtlb.mongodb.net/\"\n",
    "client = MongoClient(MONGO_URI)\n",
    "\n",
    "# Access database and collection\n",
    "db = client['spark_lab']\n",
    "transactions_col = db['transactions']\n",
    "\n",
    "# Clear existing data\n",
    "transactions_col.delete_many({})\n",
    "\n",
    "# Convert DataFrame to dictionary records\n",
    "records = df_transactions.to_dict('records')\n",
    "\n",
    "# Insert into MongoDB\n",
    "if records:\n",
    "    result = transactions_col.insert_many(records)\n",
    "    print(f\"✓ Inserted {len(result)} transaction records into MongoDB\")\n",
    "else:\n",
    "    print(\"No records to insert\")\n",
    "\n",
    "# Display sample\n",
    "sample = list(transactions_col.find().limit(3))\n",
    "print(f\"\\nSample from MongoDB:\")\n",
    "for record in sample:\n",
    "    print(f\"  Transaction ID: {record.get('Transaction ID')}, Amount: {record.get('Amount')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba497b02",
   "metadata": {},
   "source": [
    "## Section 7: Case Study - Spark Integration with MongoDB Atlas\n",
    "\n",
    "### MongoDB Setup with Your Credentials\n",
    "\n",
    "Use your MongoDB credentials to connect and analyze data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe9e801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.3 SQL Query - Successful transactions > 500\n",
    "print(\"\\n=== SQL: Successful Transactions > 500 ===\")\n",
    "result2 = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        `Transaction ID`,\n",
    "        `Sender Account ID`,\n",
    "        `Amount`,\n",
    "        `Transaction Type`,\n",
    "        `Date`\n",
    "    FROM transactions\n",
    "    WHERE `Transaction Status` = 'Success' AND `Amount` > 500\n",
    "    ORDER BY `Amount` DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "result2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e9eacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2 SQL Query - Total amount by transaction type\n",
    "print(\"\\n=== SQL: Total Amount by Transaction Type ===\")\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        `Transaction Type`, \n",
    "        SUM(`Amount`) as Total,\n",
    "        COUNT(*) as Count,\n",
    "        ROUND(AVG(`Amount`), 2) as Average\n",
    "    FROM transactions\n",
    "    GROUP BY `Transaction Type`\n",
    "    ORDER BY Total DESC\n",
    "\"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d2ac36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 Create temporary table/view\n",
    "df.createOrReplaceTempView(\"transactions\")\n",
    "print(\"✓ Temporary view 'transactions' created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805560e3",
   "metadata": {},
   "source": [
    "## Section 6: Spark SQL Operations\n",
    "\n",
    "### 6.1 Creating Temporary Views and SQL Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf654277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.7 Sorting operations - Order by amount descending\n",
    "print(\"\\n=== Top 10 Transactions by Amount (Descending) ===\")\n",
    "from pyspark.sql.functions import col, desc\n",
    "df.orderBy(desc(\"Amount\")).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb493d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.6 GroupBy aggregation - Total amount by transaction type\n",
    "print(\"\\n=== Total Amount by Transaction Type (GroupBy + Sum) ===\")\n",
    "from pyspark.sql.functions import sum as spark_sum\n",
    "df.groupBy(\"Transaction Type\").agg(spark_sum(\"Amount\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be5fe64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.5 Column selection\n",
    "print(\"\\n=== Selected Columns (ID, Type, Amount) ===\")\n",
    "df.select(\"Transaction ID\", \"Transaction Type\", \"Amount\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033716c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.4 Filtering operations - Transactions > 1000\n",
    "print(\"\\n=== Transactions with Amount > 1000 ===\")\n",
    "df.filter(df[\"Amount\"] > 1000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93695011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.3 Display schema information\n",
    "print(\"=== DataFrame Schema ===\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf25741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 Load CSV data into Spark DataFrame\n",
    "df = spark.createDataFrame(df_transactions)\n",
    "print(\"✓ Spark DataFrame created from transaction data\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6957a916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample transaction data\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "# Generate sample transaction data\n",
    "data = []\n",
    "transaction_types = [\"Purchase\", \"Transfer\", \"Withdrawal\", \"Deposit\"]\n",
    "statuses = [\"Success\", \"Failed\", \"Pending\"]\n",
    "\n",
    "for i in range(100):\n",
    "    data.append({\n",
    "        \"Transaction ID\": i + 1,\n",
    "        \"Sender Account ID\": f\"ACC{random.randint(1000, 9999)}\",\n",
    "        \"Amount\": round(random.uniform(10, 5000), 2),\n",
    "        \"Transaction Type\": random.choice(transaction_types),\n",
    "        \"Transaction Status\": random.choices(statuses, weights=[0.8, 0.1, 0.1])[0],\n",
    "        \"Date\": (datetime.now() - timedelta(days=random.randint(0, 90))).strftime(\"%Y-%m-%d\")\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "df_transactions = pd.DataFrame(data)\n",
    "print(\"✓ Sample transaction data created\")\n",
    "print(f\"Shape: {df_transactions.shape}\")\n",
    "df_transactions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c354d5da",
   "metadata": {},
   "source": [
    "## Section 5: Loading and Manipulating Data with Spark\n",
    "\n",
    "### 5.1 Creating Sample Transaction Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67ee309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.5 Create Spark Session\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ColabSpark\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"✓ Spark session created successfully!\")\n",
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e80c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.4 Configure environment and initialize Spark\n",
    "import os\n",
    "import sys\n",
    "import findspark\n",
    "\n",
    "# Configure environment (commented out - uncomment if needed)\n",
    "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
    "\n",
    "# Initialize findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "print(\"✓ findspark initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6a2a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3 Install Python packages\n",
    "!pip install -q findspark\n",
    "!pip install -q pyspark\n",
    "!pip install -q py4j\n",
    "!pip install -q pymongo matplotlib seaborn\n",
    "print(\"✓ All Python packages installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ee4997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 Download and install Apache Spark\n",
    "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
    "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
    "print(\"✓ Apache Spark downloaded and extracted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2bd580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Install Java JDK and dependencies\n",
    "!sudo apt update\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "print(\"✓ Java JDK installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb8d598",
   "metadata": {},
   "source": [
    "## Section 4: Installing PySpark on Google Colab\n",
    "\n",
    "Execute these commands in Colab cells (in order):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea7f32c",
   "metadata": {},
   "source": [
    "### 3.2 DataFrame-Based WordCount in Python\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, explode, split, count\n",
    "\n",
    "spark = SparkSession.builder.master(\"yarn\").appName('wordcount_df').getOrCreate()\n",
    "\n",
    "# Using DataFrame API (higher-level, optimized)\n",
    "lines = spark.read.text(\"hdfs://hadoop-master:9000/user/root/input/alice.txt\")\n",
    "words = lines.select(explode(split(col(\"value\"), \" \")).alias(\"word\"))\n",
    "word_counts = words.filter(col(\"word\") != \"\").groupBy(\"word\").count().orderBy(col(\"count\").desc())\n",
    "\n",
    "word_counts.show(20)\n",
    "word_counts.write.csv(\"hdfs://hadoop-master:9000/user/root/output/df_wordcount\", mode=\"overwrite\")\n",
    "spark.stop()\n",
    "```\n",
    "\n",
    "**Difference:** RDD vs DataFrame\n",
    "- **RDD:** Low-level, functional programming, manual optimization\n",
    "- **DataFrame:** High-level, SQL-like, Catalyst optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434f6b1a",
   "metadata": {},
   "source": [
    "## Section 3: Submitting Python Applications\n",
    "\n",
    "### 3.1 RDD-Based WordCount in Python\n",
    "\n",
    "Create `wordcount_rdd.py`:\n",
    "\n",
    "```python\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"yarn\").appName('wordcount').getOrCreate()\n",
    "\n",
    "# Using RDD API (lower-level)\n",
    "data = spark.sparkContext.textFile(\"hdfs://hadoop-master:9000/user/root/input/alice.txt\")\n",
    "words = data.flatMap(lambda line: line.split(\" \"))\n",
    "wordCounts = words.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)\n",
    "wordCounts.saveAsTextFile(\"hdfs://hadoop-master:9000/user/root/output/rdd_wordcount\")\n",
    "\n",
    "print(\"✓ WordCount RDD processing complete\")\n",
    "spark.stop()\n",
    "```\n",
    "\n",
    "Submit with:\n",
    "```bash\n",
    "spark-submit --master yarn wordcount_rdd.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14964f9",
   "metadata": {},
   "source": [
    "### 2.2 WordCount with Scala (spark-shell Equivalent)\n",
    "\n",
    "Execute in spark-shell (inside Docker container):\n",
    "\n",
    "```scala\n",
    "// Open spark-shell\n",
    "spark-shell\n",
    "\n",
    "// Write and execute Scala code\n",
    "val data = sc.textFile(\"hdfs://hadoop-master:9000/user/root/input/alice.txt\")\n",
    "val count = data.flatMap(line => line.split(\" \")).map(word => (word, 1)).reduceByKey(_+_)\n",
    "count.saveAsTextFile(\"hdfs://hadoop-master:9000/user/root/output/respark1\")\n",
    "\n",
    "// Exit spark-shell\n",
    ":quit\n",
    "```\n",
    "\n",
    "**Note:** This demonstrates RDD operations in Scala, showing how to:\n",
    "- Load text files from HDFS\n",
    "- Use flatMap to split lines\n",
    "- Use map/reduce to count occurrences\n",
    "- Save results back to HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c0f1d3",
   "metadata": {},
   "source": [
    "## Section 2: First Examples with Apache Spark\n",
    "\n",
    "### 2.1 SparkPi Example - Calculate PI Using spark-submit\n",
    "\n",
    "Execute the following command in your Docker container to calculate PI:\n",
    "\n",
    "```bash\n",
    "# Basic SparkPi with 100 partitions\n",
    "spark-submit \\\n",
    "  --class org.apache.spark.examples.SparkPi \\\n",
    "  --master local[*] \\\n",
    "  $SPARK_HOME/examples/jars/spark-examples_2.12-3.2.1.jar \\\n",
    "  100\n",
    "\n",
    "# Test with different partition values\n",
    "spark-submit --class org.apache.spark.examples.SparkPi --master local[*] \\\n",
    "  $SPARK_HOME/examples/jars/spark-examples_2.12-3.2.1.jar 50\n",
    "\n",
    "spark-submit --class org.apache.spark.examples.SparkPi --master local[*] \\\n",
    "  $SPARK_HOME/examples/jars/spark-examples_2.12-3.2.1.jar 500\n",
    "```\n",
    "\n",
    "**Expected Output:** PI is approximately 3.14159..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93adcb5",
   "metadata": {},
   "source": [
    "## Section 1: Installation and Setup of Spark Cluster with Docker\n",
    "\n",
    "### Docker Cluster Initialization Commands\n",
    "\n",
    "```bash\n",
    "# Start the Docker cluster\n",
    "docker-compose up -d\n",
    "\n",
    "# Access the Hadoop master container\n",
    "docker exec -it hadoop-master bash\n",
    "\n",
    "# Start Hadoop (inside the container)\n",
    "./start-hadoop.sh\n",
    "\n",
    "# Start Spark (inside the container)\n",
    "./start-spark.sh\n",
    "\n",
    "# Verify Spark and YARN are running using jps\n",
    "jps\n",
    "\n",
    "# Access web UIs\n",
    "# - Yarn Web UI: https://localhost:8088\n",
    "# - Spark Web UI: https://localhost:8080\n",
    "```\n",
    "\n",
    "After startup verification, exit the container with `exit` and continue with this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe887b37",
   "metadata": {},
   "source": [
    "# Complete Apache Spark Lab - All Pillars Covered\n",
    "## BIG DATA - ANNÉE UNIVERSITAIRE 2025-2026\n",
    "## TP: Cluster Spark avec Docker\n",
    "\n",
    "This comprehensive notebook covers all required pillars:\n",
    "- ✅ Spark Cluster installation with Docker\n",
    "- ✅ SparkPi example calculation\n",
    "- ✅ WordCount with Scala (spark-shell equivalent)\n",
    "- ✅ Python application submission\n",
    "- ✅ PySpark installation on Colab\n",
    "- ✅ CSV data loading and manipulation\n",
    "- ✅ Spark SQL operations\n",
    "- ✅ MongoDB Atlas integration\n",
    "- ✅ Data visualization with matplotlib/seaborn"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
